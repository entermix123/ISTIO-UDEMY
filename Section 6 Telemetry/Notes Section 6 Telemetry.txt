Section 6 Telemetry
===================

Section content:
----------------

20. Starting the Demo System
21. Kiali Deeper Dive
22. Kiali Dynamic Traffic Routing
23. Distributed Tracing Overview
24. Using Jaeger UI
25. Why you need to "Propagate Headers"
26. What happens if you don't propagate headers?
27. Metrics with Grafana


Optional
--------
Set alias for kubectl in bash 
	1. Add the alias: bash --> echo "alias k='kubectl'" >> ~/.bashrc
	2. Apply it: bash --> source ~/.bashrc



20. Starting the Demo System
============================

In this session we will use Istio, Kiali and Grafana. With this tools we will be able to go over all other topics in the course. As we know we deployed broken/slow application in Section 4 Hands on Demo. This time we will deploy healthy nad working application.


We will use resources in folder '1-Telemetry'
	- 1-istio-init.yaml
	- 2-istio-minikube.yaml
	- 3-kiali-secret.yaml
	- 4-label-default-namespace.yaml
	- 5-application-no-istio.yaml

Navigate to the resource folder and setup the scenario:
	Delete the old Minikube cluster
		terminal --> minikube delete
	Create new Minikube cluster
		terminal --> minikube start --cpus 4 --memory 8192 --driver docker

	Point Kubectl to Minikube cluster
		terminal --> kubectl config use-context minikube

	Test the connection
		terminal --> kubectl get nodes

		# result:
		NAME       STATUS   ROLES           AGE   VERSION
		minikube   Ready    control-plane   10s   v1.35.0


	Deply provided resources
		terminal --> cd 1-Telemetry
		
ISNTALL LATEST ISTIO - for Widnows
--------------------
Download latest istio
	bash --> curl -L https://github.com/istio/istio/releases/download/1.23.0/istio-1.23.0-win.zip -o istio.zip

	bash --> unzip istio.zip
	bash --> cd istio-1.23.0
	
Add to PATH
	bash --> export PATH="$PATH:$PWD/bin"

Reload the terminal 
	bash --> source ~/.bashrc

Confirm installation
	terminal --> istioctl version --remote=false

Install Istio
	bash --> istioctl install --set profile=demo -y

Enable sidecar injection on the working namespace (default)
	bash --> kubectl label namespace default istio-injection=enabled

Verify installation
	bash --> kubectl get pods -n istio-system

	# result:
	NAME                                    READY   STATUS    RESTARTS   AGE
	istio-egressgateway-9cc489bfc-kj4pd     1/1     Running   0          76s
	istio-ingressgateway-6f868bc4f7-qkmc6   1/1     Running   0          76s
	istiod-77cb77f5b8-7clw4                 1/1     Running   0          95s

	# all pods must be running

APPLY PROVIDED RESOURCES
------------------------
Apply the provided init file - this will modify the installed Istio installation to work with the provided resources
	bash -->  kubectl apply -f 1-istio-init.yaml

Apply the CRD 2-istio-minikube.yaml
	bash --> kubectl apply -f 2-istio-minikube.yaml

Test installation
	bash --> kubectl get pods -n istio-system

	# result:
	NAME                                   READY   STATUS    RESTARTS   AGE
	grafana-595d876fb8-f7kdf               1/1     Running   0          5m46s	# grafana runs with prometheus
	istio-egressgateway-5898456ddf-jmqgv   1/1     Running   0          5m47s	# egress gateway (outgoing traffic)
	istio-ingressgateway-7d95dbfcc-b89w4   1/1     Running   0          5m47s	# ingress gateway (incoming traffic)
	istiod-65cdd474f7-l4wmp                1/1     Running   0          5m47s	# istio deamon
	jaeger-76d7cfcfdb-b2xx9                1/1     Running   0          5m46s	# Jaeger Traces service
	kiali-594fb85dc8-s68bb                 1/1     Running   0          5m45s	# Kiali UI service
	prometheus-65bdbcd97f-ntvnw            2/2     Running   0          5m46s	# prometheus works with grafana


Apply the CRD 3-kiali-secret.yaml to prevent login requirements to Kiali UI every time
	bash --> kubectl apply -f 3-kiali-secret.yaml

	# result: secret/kiali created


Apply the CRD 4-label-default-namespace.yaml to label default namespace to be Istio enabled. 
	bash --> kubectl apply -f 4-label-default-namespace.yaml
	
	# result: namespace/default configured

	Confirm labeling
		bash --> kubectl describe ns default

		# result: Labels:       istio-injection=enabled

	# This will add the additional proxy container is every pod application in the 'default' namespace.
	# It is important to apply this label to the working namespace before start deploying the application


Deploy the application
	bash --> kubectl apply -f 5-application-no-istio.yaml

	# result:
	deployment.apps/position-simulator created
	deployment.apps/position-tracker created
	deployment.apps/api-gateway created
	deployment.apps/webapp created
	deployment.apps/vehicle-telemetry created
	deployment.apps/staff-service created
	service/fleetman-webapp created
	service/fleetman-position-tracker created
	service/fleetman-api-gateway created
	service/fleetman-vehicle-telemetry created
	service/fleetman-staff-service created

We could need some additional YAML configuration when/if we set custom routing but in this application everything is set.

List pods to confirm that every pod has 2 containers - app + proxy (istio)
	terminal --> kubectl get pods

	# result:
	NAME                                  READY   STATUS    RESTARTS   AGE
	api-gateway-64d5b6b4cb-wxc76          2/2     Running   0          3m14s
	position-simulator-8449f4c5cc-pnkxd   2/2     Running   0          3m14s
	position-tracker-676dd958cf-2v9b5     2/2     Running   0          3m14s
	staff-service-59b8695469-9sfxm        2/2     Running   0          3m14s
	vehicle-telemetry-64bb8b49-kfv6b      2/2     Running   0          3m14s
	webapp-cc469458d-rsnpl                2/2     Running   0          3m14s

We can see that all pods has 2 contaires in them. This means that proxy (Istio) container is successfully injected.
If we forget to label the namespace the proxy containers (sidecar) will not be deployed in the pods. 

If we implement Istio on existing cluster we need to apply the label for the target namespace and then restart each pod in it so they can be recreated with the additional proxy container. This can cause services downtime and we must be carefull with what Kubernetes cluster we are working with.


Expose the installed application with port forward
	terminal --> kubectl port-forward svc/fleetman-webapp 30080:80
		
	Access the application on http://127.0.0.1:30080/





21. Kiali Deeper Dive
=====================

TELEMETRY REQUIREMENTS
----------------------
1. Running (Envoy Sidecar) Proxy in each pod we wnat to monitor
2. Istion control plane need to be running (ie. istiod{istiodaemon}, kiali{UI}, jaeger{tracer}, grafana{present data})
3. We DON"T need any specific Istio YAML configuration (NO need for VirtualServices, Gateways, etc.)

Confirm requirements
	terminal --> kubectl get pods -n istio-system

	# result:
	NAME                                   READY   STATUS    RESTARTS      AGE
	grafana-595d876fb8-f7kdf               1/1     Running   1 (11h ago)   12h
	istio-egressgateway-5898456ddf-jmqgv   1/1     Running   1 (11h ago)   12h
	istio-ingressgateway-7d95dbfcc-b89w4   1/1     Running   1 (11h ago)   12h
	istiod-65cdd474f7-l4wmp                1/1     Running   1 (11h ago)   12h
	jaeger-76d7cfcfdb-b2xx9                1/1     Running   1 (11h ago)   12h
	kiali-594fb85dc8-s68bb                 1/1     Running   1 (11h ago)   12h
	prometheus-65bdbcd97f-ntvnw            2/2     Running   2 (11h ago)   12h


Check Kiali service existance
	terminal --> kubectl get svc -n istio-system

# result:
	NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                      AGE
grafana                NodePort    10.102.89.82     <none>        3000:31002/TCP                                                               12h
istio-egressgateway    ClusterIP   10.105.212.49    <none>        80/TCP,443/TCP                                                               12h
istio-ingressgateway   NodePort    10.100.233.83    <none>        15021:32300/TCP,80:31380/TCP,443:32589/TCP,31400:31000/TCP,15443:31988/TCP   12h
istiod                 ClusterIP   10.110.254.210   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        12h
jaeger-collector       ClusterIP   10.103.147.126   <none>        14268/TCP,14250/TCP                                                          12h
prometheus             ClusterIP   10.100.141.126   <none>        9090/TCP                                                                     12h
tracing                NodePort    10.99.252.72     <none>        80:31001/TCP                                                                 12h
zipkin                 ClusterIP   10.99.133.244    <none>        9411/TCP                                                                     12h

We don't have Kiali service so we need to set one in order to access Kiali feature.

Create/expose Kiali service
	terminal --> kubectl expose deployment kiali -n istio-system --type=NodePort --port=20001 --target-port=20001

	# result: service/kiali exposed


Check Kiali service existance
	terminal --> kubectl get svc -n istio-system

# result:
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                      AGE
grafana                NodePort    10.102.89.82     <none>        3000:31002/TCP                                                               12h
istio-egressgateway    ClusterIP   10.105.212.49    <none>        80/TCP,443/TCP                                                               12h
istio-ingressgateway   NodePort    10.100.233.83    <none>        15021:32300/TCP,80:31380/TCP,443:32589/TCP,31400:31000/TCP,15443:31988/TCP   12h
istiod                 ClusterIP   10.110.254.210   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        12h
jaeger-collector       ClusterIP   10.103.147.126   <none>        14268/TCP,14250/TCP                                                          12h
kiali                  NodePort    10.100.117.213   <none>        20001:30306/TCP      	# This is the created Kiali UI service                                                        14s
prometheus             ClusterIP   10.100.141.126   <none>        9090/TCP                                                                     12h
tracing                NodePort    10.99.252.72     <none>        80:31001/TCP                                                                 12h
zipkin                 ClusterIP   10.99.133.244    <none>        9411/TCP 


Expose Istio Kiali UI
	terminal --> kubectl port-forward svc/kiali 20001:20001 -n istio-system
		
	Access the Istion UI on http://localhost:20001


OVERVIEW PAGE
-------------
When we set bigger timeframe (example 30 minutes) we can see that some of the applications are not healthy. This is because when the system is started not all pods can communicate at first. If we set the timeframe to 1 minute all the warnings are gone and all applications are healthy. This is tricky moment and we can be confused and missled if we don't understand the timeframe issues.


GRAPH PAGE
----------
We need to set the namespace we want to visualize - in this case 'default'
By default "Versioned App Graph" is visualized. Choose the most easiest one for now - 'Service graph'

Service graph
-------------
Some of the specifics in the visualization:
	- Gray lines are not active in the time period we are looking over. They will be removed from the graph if no traffic is detected for some time.
	- Green lines are active traffic between services

When we click on different vihacle on the system frontend - http://127.0.0.1:30080/ , we have to wait 1-2 data updates (refresh on 15 seconds by default) events to see that the gray line is turning green again (active traffic is detected). It takes some time for the telemetry data to be picked up and analized. It is not happening immediately.

We can choose from 3 layouts from the bottom menu field. They represent the serices connections in different shape. Choose whatever fells more understandable and clear.
	- Layout 1 - left-right formation
	- Layout 2 and 3 - star formation

If the graph has many visualized service and it is hard to track they connection, we can double click on specific service/object and personal graph for this service/object will be visualized. This makes the analysis easier in heavy populated namespaces.

We also can right click on specific service and choose one of the optios:
	- Show Details - information about the pods behind the service
	- Show Traffic - information about involved objects in the communication - source -> destination
	- Show Inbound Metrics - information about request volume and duration over time


Workload Graph
--------------
Another useful display is 'Workload Graph'. Choose it and pick the layout you prefere the most.

We can open the Legend and see the objects we are working with. The difference from other graphs is the circle object. It is pods (Workload). If we have multiple pods behind one service we will still see only one circle (collective workload)!

We can get useful information by choosing 'Response Time' from Display dropdown menu.

We also can right click on specific workload and choose one of the optios: 
	- Show Details - information about workload properties, Graph Overview and Health
	- Show Traffic - information about workload destinations
	- Show Logs - We can see the logs of the pods included in the specific workload
		- We can choose specific pod and container in the dropdown menu
	- Show Inbound Metrics - information about inbound metrics
	- Show Outbound Metrics - information about request volume and duration over time

We can also enable 'Traffic Animation' from 'Display' drop down menu see how the traffic atually is directed.


Main points
-----------
	- Only recently active traffic communications are presented on the grapg page. 
	- We can set bigger time period to check more historical traffic activity.
	- If we want to activate traffic rout, we need to manually send requests true this services to visualize them.
	- Use graphs for specific service or pods to analyze it in details like requests volume, response time and logs




22. Kiali Dynamic Traffic Routing
=================================

On Graph/Service Graph page we can make changes live with Kiali. 

Case: 
-----
Leets say we have problems with 'fleetman-staff-service' - make calls to external APIs, drive the system to run additional nodes and require resources we don't have etc. We can restrict the service without deleting pods.

We can restrict specific service by using 'virtuaservices' (vs) and 'destinationrules' (dr). These are Kubernetes objects provided by Istio. 

List virtual services
	terminal --> kubectl get vs	# result: No resources found in default namespace.

List destination rules
	terminal --> kubectl get dr	# result: No resources found in default namespace.


We can create these resources with terminal or from Kiali UI.

Set traffic suspension
----------------------
Create 'virtualservice' and 'restinationrule' with Kiali UI to suspend traffic for 'fleetman-staff-service'
	- Go on Graph/Serice graph/right clik on 'fleetman-staff-service'/Show Details
	- Top right we have 'Actions' button with 3 options
		- Craete Weighted Routing
		- Create Matching Routing
		- Suspend Traffic
	- Click 'Suspend Traffic'/Create
	
Now on Graph/Service Graph we can see that 'fleetman-staff-service' service became flagged as 'Has Virtual Service'. Also 'position-simulator' workload is affected by this suspention. This is called chain failure.

List virtual services
	terminal --> kubectl get vs	

	# result: 
	NAME                     GATEWAYS   HOSTS                                                  AGE
	fleetman-staff-service              ["fleetman-staff-service.default.svc.cluster.local"]   111s

List destination rules
	terminal --> kubectl get dr

	# result: 
	NAME                     HOST                                               AGE
	fleetman-staff-service   fleetman-staff-service.default.svc.cluster.local   2m6s


We can see that 'virtualservice' (vs) and 'destinationrule' (dr) are created by Istio from Kiali UI.

This is emergency action and it is not recommended for production operation. The proper way to handle this issue is to manage YAML files manually and then apply them with proper definitive engineering approach.


We can see details for this objects on Istio Config Page in Kiali UI.
We can look at the created YAML files at:
	- Istio Config/Sfleetman-staff-service(VirtualService)/YAML
	- Istio Config/Sfleetman-staff-service(DestinationRule)/YAML

	We can use these YAML templates to manage our own Virtual Services and Destination Rules manifests. 


Restore traffic suspension
--------------------------
We will delete the traffic suspension from Kiali UI as we created it
	- Go on Graph/Serice graph/right clik on 'fleetman-staff-service'/Show Details
	- Click Actions/Delete ALL Traffic Routing
	- Click Delete

After few seconds (almost instantly) we will see that the application is working properly

After 15-30 seconds (1-2 refreshes) we can see that the traffic on the Graph/Service Graph Page is also restored.


Confirm 'virtualservice' and 'destinationrule' deletion
-------------------------------------------------------
List virtual services to confirm deletion
	terminal --> kubectl get vs	# result: No resources found in default namespace.

List destination rules to confirm deletion
	terminal --> kubectl get dr	# result: No resources found in default namespace.






23. Distributed Tracing Overview
================================

We will go over Jaeger which is distributed tracing framework that is integrated itno Istio using its telemetry data. It allows us to track a single request that we send into the system or we can get a full graph of the chain of events that the request caused. We can get detailed timing data and get detailed data information about exactly where that request went.

There are a lot of tracing frameworks that can be found on https://opentracing.io/
Two of them are supported and build in Istio:
	- Jaeger - created by Uber
	- Zipkin - created by Twitter


Check Jaeger and Zipkin services existance
	terminal --> kubectl get svc -n istio-system

# result:
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                      AGE
grafana                NodePort    10.102.89.82     <none>        3000:31002/TCP                                                               12h
istio-egressgateway    ClusterIP   10.105.212.49    <none>        80/TCP,443/TCP                                                               12h
istio-ingressgateway   NodePort    10.100.233.83    <none>        15021:32300/TCP,80:31380/TCP,443:32589/TCP,31400:31000/TCP,15443:31988/TCP   12h
istiod                 ClusterIP   10.110.254.210   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        12h
jaeger-collector       ClusterIP   10.103.147.126   <none>        14268/TCP,14250/TCP                                                          12h
kiali                  NodePort    10.100.117.213   <none>        20001:30306/TCP      	                                                       14s
prometheus             ClusterIP   10.100.141.126   <none>        9090/TCP                                                                     12h
tracing                NodePort    10.99.252.72     <none>        80:31001/TCP         # Jaeger UI service                                                          12h
zipkin                 ClusterIP   10.99.133.244    <none>        9411/TCP 	       # Zipkin UI service   


Expose Jaeger (tracing) service
	terminal --> kubectl port-forward svc/tracing 8080:80 -n istio-system
		
	We can now access Jaeger on http://localhost:8080


In Jaeger UI we can visualize events by choosing 'staff-service.default' and clicking on 'Find Traces'.

We will clik on one of the tracked vihacles in the front end of our example application and send upstream request to the Webapp pod visualized on picture (Upstream & downstream request path.png) in the session resources folder. Then downstream request (response) will be received by the Webapp pod after approximatelly 10 seconds or less. The representation of Jaeger is little bit different - it summarize overall process time in the first point where the request was sent (Upstream & downstream request Jaeger result.png).

Other terinology used in the trace processing are visualized on picture Trace-Terminology.png
	- Trace - all measured timings and overall upstram and downstream request duration
	- Span - single section of the request process timing

The exact upstream request path we can see on picture Upstream-Request-Trace.png.






24. Using Jaeger UI
===================


Expose Jaeger (tracing) service if not exposed
	terminal --> kubectl port-forward svc/tracing 8080:80 -n istio-system
		
Access Jaeger on http://localhost:8080

We need to select service in the 'Service' field to use Jaeger. Without selecting a service the button "Find Traces" isn't active.
When we select a service and click 'Find Traces' we will receive all events that this service was involved in.

Case:
-----
When we pick another vehicle (different from the current one) on the client application - http://127.0.0.1:30080/, we send request to the backend. Lets say we have unexpected delay and problem with receiving the driver's picture. We will use Jaeger to trace and find the potential problem.

We know that all requests in our example application are going true webapp pod. So in Jaeger UI (http://localhost:8080) we select webapp.default service. We should see a lot of traces. We can use the histogram to analyze which requests are slowest and investigate the reason.


Investigation process:
----------------------
Lets say we want to track slow request connected with the picture of one of the drivers. 

Configure Request filter: In Jaeger UI in 'Lookback' field we set 'Custom Time Range' and in the 'Start Time' field we set the current time, in the 'End Time' field we switch to a day ahead. By this configs we will filter requests sent in the suggested time period: one day from the current time. 

Simulate user activity: We make request by clicking one time on different driver on the application (http://127.0.0.1:30080/).

Find Traces: In Jaeger UI we click on 'Find Traces' and receive the traces of the sent request. We receive 2 traces: one for vihacle history and the other for drivers details. Made requests depends of the application implementation. Each trace has a summary with number of spans and what services its request went true. Expand one of the traces by clicking on one of them. We will see trace details visualized example on picture 'trace-details.png' in the session resource folder.

Analyze traces: We can see the request process in depth in section 'Service & Operation'. We can expand any step (span) of the request process and look over the details. Also e can select specific timeframe of the span by hover over the timeline. We can track span details as expand 'Tags' section and find the request type, target URL, response code and more details connected with the request.

(Optional) We can compare 2 or more traces by selecting them with checboxes and click 'Compare Traces'. This could be useful in specific situations.




25. Why you need to "Propagate Headers"
=======================================

TRACE FEATURE REQUIRES CHANGES ON OUR APPLICATION CODE !!!
----------------------------------------------------------

This is the only Istio feature that requires changes in our application code. The trace feature is very useful and is it worth to implement these changes and use it!

We need to set corelation for the Jaeger to connect requests in different traces. This corelation mechanism is configured with 'guid:x-request-id' field (global unique identifier) in the span/tags details presented on picture 'propagate-headers.png' in the session resource folder. This field is the same for each span in the trace. It is generated by the first proxy container (envoy/sidecar) and passed to the next one. Unfortunately we need to configure our application to pass this 'x-request-id' after pod respond to the proxy request. The logic is visualized in the picture 'guid-logic.png'.

We need to save the passed (incoming) header in variable or database and pass it with the outgoing request to the proxy container again.

If we don't set this 'x-request-id' headers to be passed by our application logic then the traces will be chunked to many smaller traces and it will be very difficult to track the requests. Istio DO NOT apprach automated headers prpagation at all!

We can find the distributed tracing requirements here - https://istio.io/latest/about/faq/#distributed-tracing.

DISTRIBUTED TRACING REQUIREMENTS WITH ISTIO
--------------------------------------------------------------
What is required for distributed tracing with Istio?

Istio enables reporting of trace spans for workload-to-workload communications within a mesh. However, in order for various trace spans to be stitched together for a complete view of the traffic flow, applications must propagate the trace context between incoming and outgoing requests.

In particular, Istio relies on applications to forward the Envoy-generated request ID, and standard headers. These headers include:

   - x-request-id
   - traceparent
   - tracestate

Zipkin users must ensure they propagate the B3 trace headers.

    x-b3-traceid
    x-b3-spanid
    x-b3-parentspanid
    x-b3-sampled
    x-b3-flags
    b3

Header propagation may be accomplished through client libraries, such as OpenTelemetry. It can also be accomplished manually, as documented in the Distributed Tracing task.
--------------------------------------------------------------


Why can't Istio propagate headers instead of the application? - https://istio.io/latest/about/faq/#distributed-tracing
---------------------------------------------------------------------
Although an Istio sidecar will process both inbound and outbound requests for an associated application instance, it has no implicit way of correlating the outbound requests to the inbound request that caused them. The only way this correlation can be achieved is if the application propagates relevant information (i.e. headers) from the inbound request to the outbound requests. Header propagation may be accomplished through client libraries or manually. Further discussion is provided in What is required for distributed tracing with Istio?.
---------------------------------------------------------------------


Building applications to support trace context propagation - https://istio.io/latest/docs/tasks/observability/distributed-tracing/overview/#building-applications-to-support-trace-context-propagation

Configure Istio for distributed tracing with Jaeger - https://istio.io/latest/docs/tasks/observability/distributed-tracing/jaeger/#configure-istio-for-distributed-tracing




26. What happens if you don't propagate headers?
================================================

This section present the test application without headers propagation configuration. 

When we test the tracing feature and make request as in the previous session, we receive the same traces but with less information. The connected spans are only to the end of the initial guid header traced. If we want to trace further the request made we need to find traces connected with the next application container that do not pass the same guid. We need to know what the next application container is and find traces for it and filter the traces and so on. 

This makes the Jaeger much less usable and time and effort consuming.






27. Metrics with Grafana
========================

We can use Grafana/Prometheus to monitor resource consumption (CPU, storage, etc.). This case is different then what we are going to look over now.

In this section we will use Grafana/Prometheus to monitor the traffic between our application services.

Find Grafana service
	terminal --> k get svc -n istio-system

# result:
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)
grafana                NodePort    10.102.89.82     <none>        3000:31002/TCP		# this is grafana service
istio-egressgateway    ClusterIP   10.105.212.49    <none>        80/TCP,443/TCP
istio-ingressgateway   NodePort    10.100.233.83    <none>        15021:32300/TCP,80:31380/TCP,443:32589/TCP,31400:31000/TCP,15443:31988/TCP
istiod                 ClusterIP   10.110.254.210   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP
jaeger-collector       ClusterIP   10.103.147.126   <none>        14268/TCP,14250/TCP
kiali                  NodePort    10.100.117.213   <none>        20001:30306/TCP
prometheus             ClusterIP   10.100.141.126   <none>        9090/TCP 
tracing                NodePort    10.99.252.72     <none>        80:31001/TCP 
zipkin                 ClusterIP   10.99.133.244    <none>        9411/TCP 


Forward grafana port
	terminal --> kubectl port-forward svc/grafana 3000:3000 -n istio-system

Access grafana on http://localhost:3000


On the top left side we can click on HOME/Istio and choose from different dashboards. We will go over the most used dashboards:
	- Istio Service Dashboard
	- Istio Workload Dashboard


Istio Workload Dashboard
------------------------
Select the correct metrics provisioner, namespace and workload:
	1. Datasource - Prometheus
	2. Namespace - default
	3. Workload - staff-service

At the top right section we can set time periods for:
	- monitored period - example Last 1 hour
	- refresh period - 60 seconds by default

Expand all section - General, Inbound Workloads and Outbound Workloads


Most useful graphs
	- Incoming Request Volume
	- Incoming Success Rate (non-5xx responses) - Healthy responses
	- Request Duration
	- Incoming Request Duration By Source - this graph will separate the requests by their source

We can view details for each graph by selecting it + v or dropdown menu/view. Escape full screen view with escape.

Incoming Request Duration By Source graph - In this graph we can see groups of requests and their durations separated by source. On the bottom we can select specific source and look over the requests from this source only.

Istio Service Dashboard
-----------------------
Select the correct metrics provisioner, namespace and workload:
	1. Datasource - Prometheus
	2. Service - fleetman-staff-service.default.svc.cluster.local
	3. Client Workload Namespace - default

In this dashboard we can see almost duplicated information from the Workload Dashboard.


Other useful Dahsboards
-----------------------
	- Istio Mesh Dashboard - Global requests metrics of our cluster
	- Istio Performance Dahsboard - We can look over Istio resource consumption
	- Istio Wasm Extension Dashboards - Advanced Istio extensions
	- Istio Control Plane Dashboard














